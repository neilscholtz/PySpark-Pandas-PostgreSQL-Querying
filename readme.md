# Overview
The purpose of this is to map query functions across PySpark, Pandas and PostgreSQL.
This will initially focus on the querying of data, then later branch out to include data transformation and ETL work. 

![pyspark](img/pyspark.jpeg) ![pandas](img/pandas.png) ![postgresql](img/postgresql.png)

# Quickstart
1. Rename `db_creds_template.py` to `db_creds.py` and update the databse information
2. Create the necessary tables for Postgres, these are the first steps included in the `SQL.ipynb` file.
3. Install PySpark locally on a Windows machine (https://bigdata-madesimple.com/guide-to-install-spark-and-use-pyspark-from-jupyter-in-windows/)

All the data used is in the `data/` folder. 
